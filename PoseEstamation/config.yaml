# general
model: "TransformerModel" 
# options "FullyConnectedNetwork",CNN1D,DLinear,TransformerModel,Conv2DLSTMAttentionModel,CNNLSTMModel
experiment_name: "omer_ZS_"
pre_trained: True
pre_trained_path: results/best/Transformer_normal/TransformerModel_epoch_8_date_25_12_09_36.pt
input_size: 32
output_size: 6

learning_rate: 0.00332 # 9.0e-04
weight_decay : 0.00749 # 2.0e-03
warmup_length : 0.365
sequence_length: 128 # 128               # Number of steps in each sequence
dropout: 0.116 # 0.2

seed: 42
epochs: 15
batch_size: 32 # 40
train_data_path: "/home/admina/pose_estamation/PoseEstamation/data/full_data"      # Directory containing CSV files
# test_data_path: "/home/admina/pose_estamation/PoseEstamation/data/test_data" 
test_data_path: "/home/admina/pose_estamation/PoseEstamation/data/multi_user/omer/test" 

model_path: "results/saved_models"
data_test_path: "data_tests_results"
num_of_data_sets_experiments: 15
wandb_on: False


train_size: 0.95                    # Proportion of data to use for training (0.8 means 80% for training)
jump: 2                            # Step size for stride sampling


# FullyConnectedNetwork
hidden_size: 256
num_layers: 2

# 1DCNN
num_filters: [32, 64, 128]
kernel_sizes: 3
use_batchnorm : true

# CNNLSTMModel
CNNLSTMModel_num_filters: [32,64,128]
CNNLSTMModel_kernel_size: 3
CNNLSTMModel_lstm_hidden_size: 128
CNNLSTMModel_lstm_num_layers: 2
CNNLSTMModel_dropout: 0.1
CNNLSTMModel_use_batchnorm: True

# Conv2DLSTMAttentionModel
conv2dlstm_num_layers: 1  # Number of LSTM layers
cnn2dlstm_dropout: 0.2  # Dropout rate
conv2d_hidden_sizes: [32,64,128] # Hidden sizes for the convolutional layers
cnn2d_kernel_size: 3  # Kernel size for the convolutional layers
cnn2dlstm_maxpool_kernel_size: 2  # Kernel size for max pooling
conv2d_n_heads: 8  # Number of attention heads
cnn2dlstm_maxpool_layers: [0,1,2]  # Layers at which to apply max pooling

# Dlinear
Dlinear_kernel_size : 25
individual : true

# Transformer Enc
d_model_transformer: 32 # 128
num_layers_transformer: 2
d_ff_transformer: 128 # 2048
transformer_n_head: 8
head_dropout_transformer: 0.01
fc_dropout_transformer: 0.25
activation: "relu"  # or "gelu" "relu"
use_learnable_positional_encoding: false  # Toggle learnable positional encoding


# # iTransformer
iTransformer_num_variates: 6
iTransformer_dim: 32                          # model dimensions
iTransformer_depth: 2                          # depth
iTransformer_heads: 8                          # attention heads
iTransformer_dim_head: 64                      # head dimension
iTransformer_pred_length: 0    # can be one prediction, or many
iTransformer_num_tokens_per_variate: 1         # experimental setting that projects each variate to more than one token. the idea is that the network can learn to divide up into time tokens for more granular attention across time. thanks to flash attention, you should be able to accommodate long sequence lengths just fine
iTransformer_use_reversible_instance_norm: True # use reversible instance normalization, proposed here https://openreview.net/forum?id=cGDAkQo1C0p . may be redundant given the layernorms within iTransformer (and whatever else attention learns emergently on the first layer, prior to the first layernorm). if i come across some time, i'll gather up all the statistics across variates, project them, and condition the transformer a bit further. that makes more sense


feature_index: ['S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 
       'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18','S19', 'S20', 
       'S21', 'S22', 'S23', 'S24', 'S25', 'S26', 'S27','S28','S29','S30','S31','S32']
label_index: [
       # 'MCx','MCy', 'MCz',
       # 'MSx', 'MSy', 'MSz',
       'MEx', 'MEy', 'MEz',
       'MWx', 'MWy', 'MWz'
]

# Fine-tuning parameters
fine_tuning:
       model: "TransformerModel_fine_tune_eden"
       pre_trained_model_path: "results/best/Transformer_normal/TransformerModel_epoch_8_date_25_12_09_36.pt"
       enabled: false
       learning_rate: 1.0e-5 
       weight_decay: 1.0e-3
       num_epochs: 15
       batch_size: 32
       criterion: "MSELoss"
       model_save_path: "fine_tuned_model.pth"
       train_data_path: "data/multi_user/omer/fine_tune"
       test_data_path: "data/multi_user/omer/test"
       layers_to_train: [
            'wrist_fc.2',
            'wrist_fc.5',
            'wrist_fc_sum',
            'elbow_fc.2', 
            'elbow_fc.5',
            'elbow_fc_sum'
        ]