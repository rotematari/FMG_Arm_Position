---
# Define the argument parse



# data headers 
# sensor arangment from the left 16 is the first  
# upper arm 0,3,2,1,
# shoulder (from the right) 20,21,25,23,22,24,8,9,6,7
# back (from the right) 4,5,15,14,13,12,10,11,31,30,29,28,27,26,16,17,19,18

clip: 0

# feature_scaler.var_>20
sensor_location: [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,17, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
fmg_index: ['S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18','S19', 'S20', 'S21', 'S22', 'S23', 'S24', 'S25', 'S26', 'S27','S28','S29','S30','S31','S32']
velocity_label_inedx: [
       # 'VCx','VCy', 'VCz','VSx', 'VSy', 'VSz', 
       'VEx', 'VEy', 'VEz', 'VWx', 'VWy', 'VWz']
session_time_stamp: ['session_time_stamp']
label_index: [
       # 'MCx','MCy', 'MCz',
       'MSx', 'MSy', 'MSz',
       'MEx', 'MEy', 'MEz',
       'MWx', 'MWy', 'MWz'
       ]
time_stamp: ['time_stamp']

model: 'TransformerModel'

models_to_train: [
       # 'DecompTransformerModel','PatchTST',
       # 'TransformerModel',Conv2DLSTMAttentionModel,TimeSeriesTransformer
       'CNN_LSTMModel',
       'TCN','DLinear',]
# data parameters 
input_size: 32
# change the code to only get the label_indexs
num_labels : 9
sample_speed : 100 #Hz


test_size: 30000
test_batch_size: 2500

random_state : 42




# hypermeters
learning_rate : 0.00005 # 
critic_lr : 0.001
dropout : 0.0 #
weight_decay : 1.2e-06 # 
critic_weight_decay : 1.0e-06 # 

window_size : 5
alpha_on_output: 0.1 # between 0 and 1
sequence_length : 16

num_epochs : 1
batch_size : 40

# base_lr : 0.000001  # Minimum learning rate
# max_lr : 0.001  # Maximum learning rate
# step_size : 1  
# gamma: 0.01
#step LR
# stable_lr: 1.0e-03 
# drop_epoch : 1

# noamLR
warmup_steps: 2000
use_schedualer: False


#for Dlinear
individual : True
pred_len: 1
use_attnproj: False
dlinear_n_heads: 8
output_channels: 1

#decomposition 
kernel_size: 45

# from convLSTM
cnn_hidden_size: 256
cnn_kernel_size: 3
maxpoll_kernel_size: 2 
lstm_hidden_size : 256
lstm_num_layers : 2

# conv2dLSTM
# the lstm gets [batch,seq,featur]
# featuremapsize = input_size // cnn2dlstm_maxpoll_kernel_size^len(cnn2dlstm_maxpoll_layers)
# so the lstm hidden size must = conv_outchannel*featuremapsize 
conv2d_hidden_sizes: [32,128,32]
cnn2dlstm_maxpoll_layers: [0,1,2]
conv2d_n_heads: 8
cnn2d_kernel_size: 3
cnn2dlstm_maxpoll_kernel_size: 2
conv2dlstm_hidden_size : 0
conv2dlstm_num_layers : 2
cnn2dlstm_dropout : 0.2

# TCN
num_channels: [28,56,112,56,28,14,9]
kernelsize_tcn : 9

#time series transformer
TST_encoder_layers : 2
TST_decoder_layers : 2
TST_n_head: 16
TST_d_ff: 1024
TST_d_model: 256

#transformer_encoder
d_model_transformer : 64
d_ff_transformer: 1048
head_dropout_transformer: 0.1
fc_dropout_transformer: 0.2
num_layers_transformer : 4
transformer_n_head: 16 

#for PatchTST
kernel_PatchTST: 25
stride: 2
patch_len : 8
n_head_PatchTST : 8
padding_patch : 'end' # default='end', help='None: None; end: padding on the end'
encoder_layers : 2
decoder_layers : 
d_model: 128
d_ff: 1024 # fullyconnected
fc_dropout : 0.1
head_dropout : 0.1
subtract_last: False
decomposition: True
revin: 1 #default=1, help='RevIN; True 1 False 0')
affine : 0 #default=0, help='RevIN-affine; True 1 False 0')

#paths
train_data_path : ./data/data
test_data_path : ./data/test
data_path: None
model_path: ./models/saved_models
best_model: ./models/saved_models/TransformerModel_epoch_0_date_17_04_13_47.pt
result_path: ./test_results



shuffle_train : True
shuffle: False

with_critic : False
loss_func : RMSELoss
EMW_on_output: True
wandb_on: False
norm: 'std'
subtract_bias: False
norm_labels: True
plot_data : False
plot_pred : False
pre_trained: False
with_velocity : False
# sequence : True

...